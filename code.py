# -*- coding: utf-8 -*-
"""Penerapan .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/149Dh3KpSDMHLkM3KqD6GFQv8PiDh66oB

**Mengakses Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')
base_path = '/content/drive/MyDrive/skripsi_sinta'

"""**Menginstal Library Tambahan**"""

!pip install scikit-multilearn
!pip install sastrawi

"""**Mengimpor Library**"""

import pandas as pd
import re
import string
import nltk
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.corpus import stopwords
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
import matplotlib.pylab as plt
import seaborn as sns
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.ensemble import RandomForestClassifier
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import KFold
import numpy as np
import joblib
from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score
from sklearn.tree import plot_tree
from mlxtend.frequent_patterns import apriori, association_rules
import warnings

# Download resource jika belum di-download
# Mengunduh resource tokenizer 'punkt' dari NLTK, diperlukan agar fungsi word_tokenize bisa digunakan
nltk.download('punkt')
nltk.download('punkt_tab')

"""**Membaca dan Membersihkan Dataset**"""

df = pd.read_csv(f"{base_path}/datalabel.csv", sep=';')

df.shape

df.head()

df.columns

df.dtypes

df.describe()

df.head(10)

df.isna().sum()

df.info()

df.duplicated().any()

df_cleaned=df.drop_duplicates()

df=df_cleaned
df.duplicated().any()

df.describe()

df.describe(exclude= np.number)

df.shape

df.isna().sum()

positive_colums = ['pelayanan positif', 'sdm positif', 'sarana dan prasarana positif']
negative_colums = ['pelayanan negatif', 'sdm negatif', 'sarana dan prasarana negatif']

# Create variables to store the counts
positive_counts = df[positive_colums].sum()
negative_counts = df[negative_colums].sum()

print('ulasan positif per aspek:')
print(positive_counts)  # Use the created variable
print('ulasan negatif per aspek')
print(negative_counts)  # Use the created variable

plt.figure(figsize=(6, 4))
plt.bar(positive_counts.index, positive_counts.values, color='green', alpha=0.5)
plt.title('Jumlah Ulasan Positif per Aspek')
plt.xlabel('Aspek')
plt.ylabel('Jumlah Ulasan')
plt.xticks(rotation=45)
plt.legend()
plt.show()

plt.bar(negative_counts.index, negative_counts.values, label='Negatif', color='red', alpha=0.5)
plt.title('Jumlah Ulasan negatif per Aspek')
plt.xlabel('Aspek')
plt.ylabel('Jumlah Ulasan')
plt.xticks(rotation=45)
plt.legend()
plt.show()

# Gabungkan data ulasan positif dan negatif ke dalam satu DataFrame
aspek = ['Pelayanan', 'SDM', 'Sarana dan Prasarana']
df_sentimen = pd.DataFrame({
    'Aspek': aspek,
    'Positif': positive_counts.values,
    'Negatif': negative_counts.values
})

# Atur ukuran figure
plt.figure(figsize=(8, 5))

# Buat barplot
df_sentimen_melted = df_sentimen.melt(id_vars='Aspek', var_name='Sentimen', value_name='Jumlah')
sns.barplot(data=df_sentimen_melted, x='Aspek', y='Jumlah', hue='Sentimen')

# Tambahkan judul dan label
plt.title('Distribusi Ulasan Positif dan Negatif per Aspek')
plt.ylabel('Jumlah Ulasan')
plt.xlabel('Aspek')

# Tampilkan plot
plt.tight_layout()
plt.show()

most_liked_aspect = positive_counts.idxmax()
most_liked_value = positive_counts.max()

most_complained_aspect = negative_counts.idxmax()
most_comlaint_value = negative_counts.max()

print(f"Aspek yang paling baik adalah: {most_liked_aspect}")
print(f"Aspek yang paling sering dikeluhkan adalah: {most_complained_aspect}")

df['text_length'] = df['snippet'].apply(lambda x: len(x.split()))

plt.hist(df['text_length'], bins=20, color='skyblue', edgecolor='black')
plt.title('distribusi panjang text')
plt.xlabel('jumlah kata per text')
plt.ylabel('frkuensi')
plt.show()

"""**Preprocessing Data**"""

# Fungsi untuk membersihkan teks dari link, angka, simbol, tanda baca, dan spasi berlebih
def cleanse_text(text):
  text = re.sub(r'http\S+|www\S+', '', text)
  text = re.sub(r'\d+', '', text)
  text = re.sub(r'[^\w\s]', '', text)
  text = text.translate(str.maketrans('', '', string.punctuation))
  text = ' '.join(text.split())
  return text

# Membuat DataFrame kosong untuk menyimpan hasil preprocessing
prepro_data = pd.DataFrame()

# Menyalin kolom 'snippet' dari dataframe df ke prepro_data
prepro_data['snippet'] = df['snippet']

# Menerapkan fungsi cleanse_text ke setiap nilai di kolom 'snippet'
# Hasilnya disimpan di kolom baru 'clean_text'
prepro_data['clean_text'] = prepro_data['snippet'].apply(cleanse_text)

prepro_data

def case_fold(text):
  return text.lower()

# Menerapkan fungsi case_fold ke setiap nilai dalam kolom 'clean_text'
# Hasilnya disimpan di kolom baru bernama 'case_fold'
prepro_data['case_fold'] = prepro_data['clean_text'].apply(case_fold)

prepro_data

def tokenize_text(text):
    return word_tokenize(text)

# Menerapkan fungsi tokenisasi ke setiap nilai dalam kolom 'case_fold'
# Hasil tokenisasi disimpan dalam kolom baru bernama 'tokenizing'
prepro_data['tokenizing'] = prepro_data['case_fold'].apply(tokenize_text)

prepro_data

nltk.download('stopwords')

def remove_stopwords(tokens):
  # Mendapatkan daftar stopwords bahasa Indonesia dari NLTK
  stop_words = set(stopwords.words('indonesian'))

  # Mengembalikan daftar token yang bukan merupakan stopword
  return [word for word in tokens if word not in stop_words]

# Menerapkan fungsi remove_stopwords ke kolom 'tokenizing'
# Hasilnya disimpan di kolom baru bernama 'stopword'
prepro_data['stopword'] = prepro_data['tokenizing'].apply(remove_stopwords)

prepro_data

def load_slang_xlsm(path, sheet_name=0):
    df = pd.read_excel(path, sheet_name=sheet_name)

    # Mengubah dua kolom ('slang' dan 'formal') menjadi dictionary: {'slg': 'formal'}
    slang_dict = dict(zip(df['slang'], df['formal']))

    # Mengembalikan kamus slangword
    return slang_dict

def slangword_conversion(tokens, slang_dict):
    # Jika kata ada dalam kamus slang, ganti dengan padanan formal
    # Jika tidak ada, biarkan tetap seperti aslinya
    return [slang_dict.get(word, word) for word in tokens]

slang_dict = load_slang_xlsm(f"{base_path}/Slangword-indonesian.xlsm")

# Menerapkan fungsi slangword_conversion ke kolom 'stopword'
# Untuk setiap baris, token akan diubah ke versi formal jika ada di kamus slang
# Hasilnya disimpan di kolom baru bernama 'slangword'
prepro_data['slangword'] = prepro_data['stopword'].apply(lambda x: slangword_conversion(x, slang_dict))

prepro_data

# Inisialisasi objek factory dari library Sastrawi untuk membuat stemmer
factory = StemmerFactory()

stemmer = factory.create_stemmer()

def stemming_text(tokens):
    return [stemmer.stem(word) for word in tokens]

# Menerapkan fungsi stemming ke kolom 'slangword'
# Setiap kata dalam token akan diubah ke bentuk dasar (stem)
# Hasilnya disimpan di kolom baru 'stemming'
prepro_data['stemming'] = prepro_data['slangword'].apply(stemming_text)

prepro_data

# Menggabungkan kembali list kata-kata hasil stemming menjadi satu string
prepro_data['hasil_akhir'] = prepro_data['stemming'].apply(lambda x: ' '.join(x))

prepro_data

# Menyimpan ke CSV
prepro_data.to_csv('datalabel_prepro.csv', index=False)

# Memisahkan fitur dan label dari dataset

# Baris ini sebelumnya digunakan untuk mengambil kolom pertama dari df sebagai fitur (teks ulasan asli)
# Namun sekarang sudah tidak dipakai karena kita ingin menggunakan hasil preprocessing
# X = df.iloc[:, 0]

# Mengambil kolom 'hasil_akhir' dari dataframe prepro_data sebagai fitur (teks yang sudah dibersihkan)
X = prepro_data['hasil_akhir']

# Mengambil semua kolom label dari dataframe df, dimulai dari kolom ke-3 hingga akhir
# Ini karena kolom ke-1 adalah snippet, kolom ke-2 kemungkinan adalah teks bersih, dan sisanya adalah label (multi-label)
Y = df.iloc[:, 1:]
# df.iloc[:, 2:].isna().sum()

# Menampilkan isi dari Y (label)
Y

"""**TF-IDF**"""

# Membuat vektorisasi teks dengan TF-IDF
vectorizer = TfidfVectorizer()                     # Inisialisasi objek TF-IDF
X_vectorized = vectorizer.fit_transform(X)         # Mengubah teks hasil preprocessing menjadi fitur numerik

#mengkonfersi sparse matrix ke dataframe pandas
df_tfidf = pd.DataFrame(X_vectorized.toarray(), columns=vectorizer.get_feature_names_out())

#mengatur index dataframe berupa label dengan index menjadi kalimat 1 dan seterusnya
df_tfidf.index = [f"Kalimat {i+1}" for i in range(len(X))]

#menyimpan dataframe ke excel
df_tfidf.to_excel("hasil_tfidf_full.xlsx", index=True)

joblib.dump(vectorizer, 'tfidf_vectorizer_model.pkl')

# Menampilkan hasil TF-IDF per kata untuk setiap kalimat
for idx, row in df_tfidf.iterrows():
    #menampilkan setiap indek kalimat seperti kalimat 1
    print(f"\n{idx}:")
    for word, score in row.items():
        #menampilkan kata yang nilai TF-IDFnya lebih dari 0
        if score > 0:
            #menampilkan nilai TF-IDF perkata dari setiap kalimat
            print(f"  {word}: {score:.4f}")

"""**Modeling dan evaluasi**"""

# Konversi Y ke array
Y_array = Y.values

# Daftar jumlah pohon yang diuji
rf_trees_uji = [20, 40, 60, 80, 100]

# List untuk menyimpan semua hasil evaluasi
results_depth25 = []

# Variabel untuk menyimpan model terbaik berdasarkan micro F1
best_micro_f1 = -1  # Karena semakin besar lebih baik

# Loop untuk masing-masing jumlah pohon
for n_trees in rf_trees_uji:
    kf = KFold(n_splits=10, shuffle=True, random_state=42)

    hamming_losses = []
    micro_f1s = []
    micro_precisions = []
    micro_recalls = []

    fold_results = {'rf_trees': n_trees, 'max_depth': 25}

    # Bersihkan data dari NaN
    nan_rows = np.isnan(Y_array).any(axis=1)
    X_vectorized_cleaned = X_vectorized[~nan_rows]
    Y_array_cleaned = Y_array[~nan_rows]

    for fold, (train_index, test_index) in enumerate(kf.split(X_vectorized_cleaned), start=1):
        X_train, X_test = X_vectorized_cleaned[train_index], X_vectorized_cleaned[test_index]
        Y_train, Y_test = Y_array_cleaned[train_index], Y_array_cleaned[test_index]

        # Inisialisasi classifier dengan max_depth=25
        classifier = BinaryRelevance(RandomForestClassifier(
            n_estimators=n_trees,
            max_depth=25,
            class_weight="balanced",
            random_state=42
        ))

        classifier.fit(X_train, Y_train)
        Y_pred = classifier.predict(X_test)

        Y_pred_array = Y_pred.toarray() if hasattr(Y_pred, "toarray") else np.array(Y_pred)
        Y_test_array = Y_test.toarray() if hasattr(Y_test, "toarray") else np.array(Y_test)

        Y_pred_array = (Y_pred_array > 0).astype(int)
        Y_test_array = (Y_test_array > 0).astype(int)

        # Evaluasi metrik
        loss = hamming_loss(Y_test_array, Y_pred_array)
        micro_f1 = f1_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)
        micro_precision = precision_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)
        micro_recall = recall_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)

        # Simpan nilai per fold
        hamming_losses.append(loss)
        micro_f1s.append(micro_f1)
        micro_precisions.append(micro_precision)
        micro_recalls.append(micro_recall)

        fold_results[f'hamming_loss_fold_{fold}'] = loss
        fold_results[f'micro_f1_fold_{fold}'] = micro_f1
        fold_results[f'micro_precision_fold_{fold}'] = micro_precision
        fold_results[f'micro_recall_fold_{fold}'] = micro_recall

    # Hitung rata-rata semua metrik
    avg_hamming_loss = np.mean(hamming_losses)
    avg_micro_f1 = np.mean(micro_f1s)
    avg_micro_precision = np.mean(micro_precisions)
    avg_micro_recall = np.mean(micro_recalls)

    fold_results['avg_hamming_loss'] = avg_hamming_loss
    fold_results['avg_micro_f1'] = avg_micro_f1
    fold_results['avg_micro_precision'] = avg_micro_precision
    fold_results['avg_micro_recall'] = avg_micro_recall

    results_depth25.append(fold_results)

    # Simpan model terbaik berdasarkan Micro F1
    if avg_micro_f1 > best_micro_f1:
        best_micro_f1 = avg_micro_f1
        joblib.dump(classifier, 'model_terbaik_f1score.pkl')
        print(f"Model terbaik disimpan dengan Micro F1: {best_micro_f1:.4f} | Pohon: {n_trees}")

# Atur opsi tampilan pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

# Tampilkan hasil evaluasi
df_depth25 = pd.DataFrame(results_depth25)
print("\nHasil Evaluasi dengan max_depth = 25:")
display(df_depth25)

# Konversi Y ke array
Y_array = Y.values

# Daftar nilai max_depth yang diuji
max_depth_values = [5, 10, 15, 20, 25]

# List untuk menyimpan semua hasil evaluasi
results_depth_variasi = []

# Loop untuk masing-masing max_depth dengan jumlah pohon tetap 100
for depth in max_depth_values:
    #Membagi data ke dalam 10 bagian (fold) untuk validasi silang (cross-validation)
    kf = KFold(n_splits=10, shuffle=True, random_state=42)

    #menyimpan hasil evaluasi per fold
    hamming_losses = []
    micro_f1s = []
    micro_precisions = []
    micro_recalls = []

    #Dictionary yang menyimpan semua hasil evaluasi untuk satu konfigurasi jumlah pohon
    fold_results = {'rf_trees': 100, 'max_depth': depth}

    #Mengambil data train dan test berdasarkan pembagian fold dari KFold
    for fold, (train_index, test_index) in enumerate(kf.split(X_vectorized), start=1):
        X_train, X_test = X_vectorized[train_index], X_vectorized[test_index]
        Y_train, Y_test = Y_array[train_index], Y_array[test_index]

        # RandomForest dengan max_depth bervariasi
        classifier = BinaryRelevance(RandomForestClassifier(
            n_estimators=100,
            max_depth=depth,
            class_weight="balanced",
            random_state=42
        ))

        #melatih model
        classifier.fit(X_train, Y_train)
        #Melakukan prediksi (predict) terhadap data test
        Y_pred = classifier.predict(X_test)

        #Mengubah hasil prediksi dan data asli ke array biasa dan Mengonversi semua nilai ke 0 dan 1 (jika perlu), untuk evaluasi metrik klasifikasi
        Y_pred_array = Y_pred.toarray() if hasattr(Y_pred, "toarray") else np.array(Y_pred)
        Y_test_array = Y_test.toarray() if hasattr(Y_test, "toarray") else np.array(Y_test)

        Y_pred_array = (Y_pred_array > 0).astype(int)
        Y_test_array = (Y_test_array > 0).astype(int)

        # Evaluasi metrik
        loss = hamming_loss(Y_test_array, Y_pred_array)
        micro_f1 = f1_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)
        micro_precision = precision_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)
        micro_recall = recall_score(Y_test_array, Y_pred_array, average='micro', zero_division=0)

        #Menggabungkan semua label dan menghitung metrik secara global
        hamming_losses.append(loss)
        micro_f1s.append(micro_f1)
        micro_precisions.append(micro_precision)
        micro_recalls.append(micro_recall)

        # Simpan nilai per fold
        fold_results[f'hamming_loss_fold_{fold}'] = loss
        fold_results[f'micro_f1_fold_{fold}'] = micro_f1
        fold_results[f'micro_precision_fold_{fold}'] = micro_precision
        fold_results[f'micro_recall_fold_{fold}'] = micro_recall

    # Simpan rata-rata hasil evaluasi semua fold
    fold_results['avg_hamming_loss'] = np.mean(hamming_losses)
    fold_results['avg_micro_f1'] = np.mean(micro_f1s)
    fold_results['avg_micro_precision'] = np.mean(micro_precisions)
    fold_results['avg_micro_recall'] = np.mean(micro_recalls)

    results_depth_variasi.append(fold_results)

# Atur opsi tampilan pandas untuk menampilkan semua kolom dan baris
pd.set_option('display.max_columns', None)  # Menampilkan semua kolom
pd.set_option('display.max_rows', None)     # Menampilkan semua baris
pd.set_option('display.width', None)        # Menghindari wrapping output
pd.set_option('display.max_colwidth', None) # Menampilkan seluruh isi kolom

# Tampilkan hasil evaluasi sebagai DataFrame
df_depth_variasi = pd.DataFrame(results_depth_variasi)
print("\nHasil Evaluasi dengan n_estimators = 100 dan variasi max_depth:")
print(df_depth_variasi)

#loop setiap label (dengan Binary Relevance)
#Dari setiap model label (Random Forest), dengan mengamil 1 decision tree
#Tree tersebut divisualisasikan dengan informasi fitur dan nama kelas
#Output: Satu gambar decision tree untuk setiap label

#aspek atau kategori yang mewakili masing-masing label di dataset
label_names = ["pelayanan positif", "pelayanan negatif",
               "sdm positif", "sdm negatif",
               "sarana dan prasarana positif", "sarana dan prasarana negatif"]


# Visualisasi 1 pohon pertama dari setiap label (BinaryRelevance)
for i, rf_model in enumerate(classifier.classifiers_):
    #Jika jumlah classifier melebihi jumlah nama label yang tersedia, maka hentikan loop dan tampilkan peringatan
    if i >= len(label_names):
        print(f"Label index {i} melebihi panjang label_names.")
        break

    # Ambil 1 pohon dari Random Forest model per label
    tree_model = rf_model.estimators_[0]

    # Deteksi class-names otomatis
    class_names = [str(cls) for cls in tree_model.classes_]

    # Plot visualisasi
    plt.figure(figsize=(40, 20))
    plot_tree(tree_model,
              #nama fitur (kata-kata) dari TF-IDF vectorizer
              feature_names=vectorizer.get_feature_names_out(),
              #nama kelas 0 atau 1
              class_names=class_names,
              filled=True,
              rounded=True)
    plt.title(f"Visualisasi Decision Tree - Label: {label_names[i]}")
    #layout agar tidak tumpang tindih
    plt.tight_layout()
    plt.show()

for i, rf_model in enumerate(classifier.classifiers_):
    if i >= len(label_names):
        print(f"Label index {i} melebihi panjang label_names.")
        break

    tree_model = rf_model.estimators_[0]
    class_names = [str(cls) for cls in tree_model.classes_]

    plt.figure(figsize=(40, 20))  # Ukuran besar
    plot_tree(tree_model,
              feature_names=vectorizer.get_feature_names_out(),
              class_names=class_names,
              filled=True,
              rounded=True)
    plt.title(f"Visualisasi Decision Tree - Label: {label_names[i]}")
    plt.tight_layout()
    plt.savefig(f"tree_label_{i}.png", dpi=300)  # Simpan ke file
    plt.close()

"""**Support, Confidence, Lift**"""

#Mengabaikan warning dari tipe RuntimeWarning agar output bersih
warnings.filterwarnings("ignore", category=RuntimeWarning)

# Konversi array ke DataFrame boolean
Y_df = pd.DataFrame(Y_array, columns=Y.columns).astype(bool)

# Kelompokkan label ke dalam dua kategori agar bisa diberikan threshold yang berbeda untuk support dan confidence
group_positif = ['pelayanan positif', 'sdm positif', 'sarana dan prasarana positif']
group_negatif = ['pelayanan negatif', 'sdm negatif', 'sarana dan prasarana negatif']

# Simpan hasil ke dalam dictionary
label_tables = {}

# perulangan setiap label
for label in Y_df.columns:
    result_matrix = []

    # Tentukan threshold berdasarkan kelompok
    if label in group_positif:
        min_supports = list(range(5, 45, 5))     # 5% - 40%
        min_confidences = list(range(10, 90, 10))  # 10% - 80%
    elif label in group_negatif:
        min_supports = list(range(1, 20, 5))     # 1% - 40%
        min_confidences = list(range(1, 30, 5))   # 6% - 40%
    else:
        min_supports = list(range(5, 45, 5))
        min_confidences = list(range(5, 105, 5))

    # Loop kombinasi support-confidence
    for conf in min_confidences:
        row_result = []
        for supp in min_supports:
            #Mengambil frequent itemsets (kombinasi label yang sering muncul) dengan min_support
            freq_items = apriori(Y_df, min_support=supp / 100, use_colnames=True)

            #Jika tidak ditemukan itemset, langsung tambahkan 0 ke hasil baris
            if freq_items.empty:
                row_result.append(0)
                continue

            #Buat aturan asosiasi dari itemsets yang ditemukan, berdasarkan confidence
            rules = association_rules(freq_items, metric="confidence", min_threshold=conf / 100)
            #Filter aturan yang menjadikan label ini sebagai konsekuen
            filtered_rules = rules[rules['consequents'].apply(lambda x: label in x)]
            #Simpan jumlah aturan yang valid untuk kombinasi support-confidence ini
            row_result.append(len(filtered_rules))

        #Simpan baris hasil ke dalam matrix total
        result_matrix.append(row_result)

    # Buat DataFrame hasilnya
    df_result = pd.DataFrame(result_matrix,
                             index=[f"{c}%" for c in min_confidences],
                             columns=[f"{s}%" for s in min_supports])

    label_tables[label] = df_result

# Tampilkan tabel hasil
for label, df in label_tables.items():
    print(f"\nTabel Jumlah Rule untuk Aspek: {label}")
    print(df)

# Inisialisasi dictionary untuk menyimpan rata-rata lift per label
avg_lift_per_label = {}

# ulangi setiap label (aspek)
for label in Y_df.columns:
    # Tentukan parameter support & confidence berdasarkan kelompok
    if label in group_positif:
        min_support = 0.05  # 5%
        min_conf = 0.10     # 10%
    elif label in group_negatif:
        min_support = 0.01  # 1%
        min_conf = 0.06     # 6%
    else:
        min_support = 0.05
        min_conf = 0.10

    # Menerapkan algoritma Apriori untuk mendapatkan itemset yang sering muncul dengan minimum support yang sudah ditentukan
    freq_items = apriori(Y_df, min_support=min_support, use_colnames=True)

    # Jika tidak ada itemset yang ditemukan, diberi nilai rata-rata lift 0.0
    if freq_items.empty:
        avg_lift_per_label[label] = 0.0
        continue

    # Buat aturan asosiasi berdasarkan confidence minimum
    rules = association_rules(freq_items, metric="confidence", min_threshold=min_conf)

    # Filter aturan yang menjadikan label sebagai consequent
    filtered_rules = rules[rules['consequents'].apply(lambda x: label in x)]

    # Hitung rata-rata lift
    if not filtered_rules.empty:
        avg_lift = filtered_rules['lift'].mean()
    #Jika tidak ada aturan (kosong), set avg_lift = 0.0
    else:
        avg_lift = 0.0

    #Simpan nilai rata-rata lift ke dictionary untuk label saat ini, dibulatkan 2 desimal
    avg_lift_per_label[label] = round(avg_lift, 2)

# Konversi hasil ke DataFrame
df_lift = pd.DataFrame({
    'Aspek': list(avg_lift_per_label.keys()),
    'Nilai Rata-rata Lift Ratio': list(avg_lift_per_label.values())
})

# Tampilkan hasil akhir
print("\nTabel Nilai Rata-rata Lift Ratio pada Setiap Aspek:\n")
print(df_lift.to_string(index=False))